# Annotated Transformer

This is an [annotated paper](https://github.com/benjaminbeilharz/annotated-transformer/blob/main/Attention_is_all_you_need_%E2%80%93_WS2021_22.ipynb) of the Transformer architecture implemented in numpy to explain the main mechanisms of self-attention and multihead attention to the students of the attention seminar.

## TODO:
- [x] add JAX/autograd
