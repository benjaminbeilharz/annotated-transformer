# Annotated Transformer

This is an [annotated paper](https://github.com/benjaminbeilharz/annotated-transformer) of the Transformer architecture implemented in numpy to explain the main mechanisms of self-attention and multihead attention to the students of the attention seminar.

## TODO:
- [ ] add JAX/autograd
